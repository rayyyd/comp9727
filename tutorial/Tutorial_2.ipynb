{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pbw64x46dX49"
      },
      "source": [
        "# **COMP9727 Recommender Systems**\n",
        "## Tutorial Week 2: Topic Classification\n",
        "\n",
        "@Author: **Mingqin Yu**\n",
        "\n",
        "@Reviewer: **Wayne Wobcke**\n",
        "\n",
        "### Objective\n",
        "\n",
        "The aim of the tutorial is to become more familiar with text processing pipelines for use in topic classification and content-based recommendation, focusing on Naive Bayes models for text classification. This will be important for the assignment, where this will form the basis of a recommender system.\n",
        "\n",
        "### Before the tutorial\n",
        "\n",
        "1. Look up regex, in particular the regular expression syntax. This will be used to identify \"words\" in the documents prior to classification.\n",
        "\n",
        "2. Review the lecture material on Naive Bayes text classification and metrics for evaluation of classification models.\n",
        "\n",
        "3. Read, understand and run all the code in the NLP pipeline below, and come prepared to discuss the answers to some of the questions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrXoTPuL87-p"
      },
      "source": [
        "### Overview of NLP Pipeline\n",
        "\n",
        "1. **Loading the Dataset**\n",
        "    - Load a subset of the '20 newsgroups' dataset\n",
        "2. **Initial Data Cleansing**\n",
        "    - Remove duplicate and missing values\n",
        "2. **Data Preprocessing**\n",
        "    - Convert the text to lower case\n",
        "    - Remove punctuation and other characters\n",
        "    - Tokenize the text\n",
        "    - Remove stopwords\n",
        "    - Stem the words\n",
        "3. **Feature Extraction**\n",
        "    - Convert the preprocessed text data into tf-idf values\n",
        "4. **Construct the Training and Test Sets**\n",
        "    - Split the data into 80% training and 20% test sets\n",
        "5. **Model Training**:\n",
        "    - Train a Naive Bayes model on the training data\n",
        "6. **Model Evaluation**\n",
        "    - Predict the categories of instances in the test set\n",
        "    - Evaluate the model's performance using accuracy and the classification report\n",
        "\n",
        "**Specific Learning Objectives**\n",
        "\n",
        "1. **Understand and Implement Data Preprocessing**\n",
        "    - Understand the importance of data preprocessing in Natural Language Processing tasks\n",
        "    - Implement text preprocessing steps such as lowercasing, removing punctuation, tokenization, removing stopwords and stemming\n",
        "2. **Understand and Implement Text Vectorization**\n",
        "    - Understand the use of tf-idf (Term Frequency-Inverse Document Frequency) vectors\n",
        "    - Implement text vectorization using the `TfidfVectorizer` from `sklearn.feature_extraction.text`\n",
        "3. **Understand and Implement Model Training and Evaluation**\n",
        "    - Understand the basics of Naive Bayes classifiers\n",
        "    - Implement model training using the `BernoulliNB` class from `sklearn.naive_bayes`\n",
        "    - Evaluation the model using `accuracy_score` and `classification_report` from `sklearn.metrics`\n",
        "4. **Understand and Implement Data Splitting**:\n",
        "    - Understand the importance of splitting the data into training and test sets\n",
        "    - Implement data splitting using the `train_test_split` function from `sklearn.model_selection`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6M4LJ_S0d5g1"
      },
      "source": [
        "## NLP Pipeline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BS9MX9WUS-fp"
      },
      "source": [
        "### 1. Introduction\n",
        "\n",
        "Text classification is a crucial task in Natural Language Processing (NLP) with various applications such as sentiment analysis, spam detection and topic classification. This tutorial will guide you through the process of building a text classification model using the Bernoulli Naive Bayes algorithm. We will start with text preprocessing and feature extraction, followed by model training and evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bn5CwHANJ2cO"
      },
      "source": [
        "### 2. Data Preprocessing\n",
        "\n",
        "Raw text data is unstructured and noisy. It contains a lot of unwanted characters, punctuation and stopwords. Data preprocessing is essential to clean and structure the text data before feeding it into the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgHjJeKQIL6n"
      },
      "source": [
        "__Step 1. Load the Dataset__\n",
        "\n",
        "* The first step is to load the data. We are using the '20 newsgroups' dataset, which is a collection of approximately 20,000 newsgroup documents, spanning 20 different newsgroups. We fetch a small subset of the dataset with only three categories: 'rec.autos', 'sci.med', 'comp.graphics'.\n",
        "* The `fetch_20newsgroups` function from `sklearn.datasets` is used to fetch the dataset. The data is put into a Python dataframe.\n",
        "* Next we drop any duplicate rows and rows with missing values from the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OP_hPmjeTzMw"
      },
      "outputs": [],
      "source": [
        "# Loading the Data\n",
        "# Check the sklearn documentation for details\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import pandas as pd\n",
        "\n",
        "# Fetch a subset of the 20 newsgroups dataset\n",
        "categories = ['rec.autos', 'sci.med', 'comp.graphics']\n",
        "newsgroups = fetch_20newsgroups(subset='train', categories=categories)\n",
        "\n",
        "# Create a dataframe\n",
        "df = pd.DataFrame({'Content': newsgroups.data, 'Category': newsgroups.target})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVdkXrfyIeKc",
        "outputId": "c2dd71d0-d6e4-49be-a911-12460693961c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                                                                                                        Content  Category\n",
            "0   bbsmiragetsoftnet jerri lee subject cobra 20 1b1 video card help organ tsoft bb public access unix 1 415 969 8238 line 22 anyon netland ...         0\n",
            "1   tedebearlelandstanfordedu theodor chen subject bmw worth price organ dsg stanford univers ca 94305 usa line 19 articl 1993apr51351531113...         1\n",
            "2   weltycabotballtowncmacom richard welti subject recauto automot mail list keyword monthli post replyto weltyballtowncmacom organ new york...         1\n",
            "3   mchaffeedclnxt07 michael chaffe subject manual shift bigot organ univers illinoi urbana line 21 eliotlanmolaengrwashingtonedu eliot writ...         1\n",
            "4   zyehcaspianuscedu zhenghao yeh subject point within polygon organ univers southern california lo angel ca line 28 distribut world nntppo...         0\n",
            "5   cfaksux1ctseiuedu alic sander subject frozen shoulder lawn mow organ eastern illinoi univers line 12 ihav frozen shoulder year year stil...         2\n",
            "6   c23regkocrsv01delcoelectcom ron gaskin subject dumbest automot concept tim origin c23regkoptsw21 keyword dimmer switch locat repost orga...         1\n",
            "7   dstampepsychtorontoedu dave stamp subject fast polygon routin need keyword polygon need organ depart psycholog univers toronto line 27 s...         0\n",
            "8   samsonprlhp1prlphilipscouk mark samson subject psygnosi cdi titl rumour 3do replyto samsonprlhp1uucp mark samson organ philip research l...         0\n",
            "9   brandtcsuncedu andrew brandt subject 4runner pathfind recent chang organ univers north carolina chapel hill line 9 nntppostinghost axonc...         1\n",
            "10  zorropicassoocistempleedu john grabowski subject tauruss rotor recal organ templ univers line 23 nntppostinghost picassoocistempleedu xn...         1\n",
            "11  lehraustinibmcom ted lehr subject scienc methodolog homeopathi tradit origin lehrjanaustinibmcom distribut inet organ ibm austin line 47...         2\n",
            "12  shmuelmapsuteinsteincom shmuel einstein subject screen captur cymk convert nntppostinghost mapsuteinsteincom organ shmuel einstein assoc...         0\n",
            "13  schultzschultzkgnibmcom karl schultz subject vesa standard vgasvga program replyto schultzvnetibmcom organ ibm aw graphic system keyword...         0\n",
            "14  bjonestrentuca name subject warningpleas read replyto bjonestrentuca organ trent univers peterborough line 26 articl 1993apr160919386821...         1\n",
            "15  snicholsadobecom sherri nichol subject exercis migrain articleid adobe1993apr1522404915516 organ adob system incorpor line 12 articl 199...         2\n",
            "16  rmt6rfaradayclasvirginiaedu roy matthew thigpen subject chrysler new yorker lh chrysler compact lh sedan organ univers virginia line 7 w...         1\n",
            "17  subject expert penicillinlook ndacumoeiscalstateedu noah dacumo organ calif state univelectron inform servic line 8 name noah dacumo stu...         2\n",
            "18  neidecknestvxenetdeccom burkhard neideckerlutz subject rumour 3do organ cec karlsruh line 17 nntppostinghost nestvx articl 1993apr151649...         0\n",
            "19  marklukecraycom mark dean subject ford auto nntppostinghostnntpd29970 lukenavonavymil replyto crayce1popsnavonavymil organ cray research...         1\n"
          ]
        }
      ],
      "source": [
        "pd.set_option('display.width', 180)\n",
        "pd.set_option('display.max_colwidth', 140)\n",
        "print(df.head(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3kzfaBe6OmZ"
      },
      "source": [
        "__Step 2. Initial Data Cleansing__\n",
        "\n",
        "Duplicate and missing values need to be removed before training the model, though there should be none in this dataset.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PkHBgGVl6XK7"
      },
      "outputs": [],
      "source": [
        "# Drop duplicates and missing values\n",
        "df = df.drop_duplicates()\n",
        "df = df.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QcaZbE-J3ID",
        "outputId": "a1e61f01-d853-4120-c803-b2eeef18d7ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 1772 entries, 0 to 1771\n",
            "Data columns (total 2 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   Content   1772 non-null   object\n",
            " 1   Category  1772 non-null   int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 41.5+ KB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "print(df.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAGgGeSI6fux"
      },
      "source": [
        "__Step 3. Text Preprocessing__\n",
        "\n",
        "The raw text data is unstructured and contains a lot of noise such as punctuation, stopwords and different cases. Preprocessing the text data helps in removing this noise and converting the text into a structured form. Here the text data is preprocessed using the Natural Language Toolkit (NLTK). A preprocessing function `preprocess_text` is defined which carries out all these preprocessing steps. This function is then applied to each document in the dataframe.\n",
        "\n",
        "1. **Regular Expressions (`re` library)**: Regular expressions are used for specifying text patterns. We use the `re.sub` function from the `re` library to remove punctuation and special characters from the text. This is important because those characters might introduce noise into the data. On the other hand, some of these characters might be useful for classification.\n",
        "\n",
        "2. **Lowercasing**: This process involves converting all the characters in the text to lowercase using the `lower` function. This step can help in treating words like 'The' and 'the' as the same word, which is usually the desired behaviour in text analysis. However, some information pertaining to proper names might be lost, such as when 'Apple' is reduced to 'apple'.\n",
        "\n",
        "3. **Tokenization**: This is the process of splitting text into words (tokens) using the `word_tokenize` function from the `nltk.tokenize` module. This is a fundamental step because text data needs to be tokenized into words for analysis.\n",
        "\n",
        "4. **Stopword Removal**: Stopwords are common words in a language (e.g. 'the', 'a', 'and', etc.) that are usually removed from the text during preprocessing. Here we use the `stopwords.words` function from the `nltk.corpus` module to get a list of stopwords in English. This step is crucial because it helps in focusing on the important words by removing the most common words that do not carry much information, similar to the use of tf-idf features.\n",
        "\n",
        "5. **Stemming/Lemmatization**: This process involves converting a word to a shortened form, here using the `PorterStemmer` class from the `nltk.stem` module. This can help reduce the dimensionality of the text data and may result in more meaningful words, however may also combine different words with the same stem, losing the distinction between them, such as with 'machine' and 'machinery'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LIM1gSaQpg_",
        "outputId": "6b86d3e0-fc91-4110-f7b3-12258851d768"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/wobcke/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /Users/wobcke/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "ps = PorterStemmer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Define preprocessing function\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)     # Check what this removes --- might be too much!\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    tokens = [ps.stem(word) for word in tokens]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Apply preprocessing to each document\n",
        "df['Content'] = df['Content'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kzFA4CcYKaJ3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0    bbsmiragetsoftnet jerri lee subject cobra 20 1b1 video card help organ tsoft bb public access unix 1 415 969 8238 line 22 anyon netland ...\n",
            "1    tedebearlelandstanfordedu theodor chen subject bmw worth price organ dsg stanford univers ca 94305 usa line 19 articl 1993apr51351531113...\n",
            "2    weltycabotballtowncmacom richard welti subject recauto automot mail list keyword monthli post replyto weltyballtowncmacom organ new york...\n",
            "3    mchaffeedclnxt07 michael chaffe subject manual shift bigot organ univers illinoi urbana line 21 eliotlanmolaengrwashingtonedu eliot writ...\n",
            "4    zyehcaspianuscedu zhenghao yeh subject point within polygon organ univers southern california lo angel ca line 28 distribut world nntppo...\n",
            "Name: Content, dtype: object\n"
          ]
        }
      ],
      "source": [
        "print(df['Content'].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Model Training and Evaluation\n",
        "\n",
        "Now the model can be constructed. The first step is feature extraction, in this case converting each document to a vector of tf-idf values, followed by training a Bernoulli Naive Bayes model and evalation of the model using various metrics and plots."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rC0ZxtW8Q4Ly"
      },
      "source": [
        "__Step 1. Feature Extaction__\n",
        "\n",
        "Here tf-idf features are used to convert each document into a numerical vector using the `TfidfVectorizer` function from `sklearn.feature_extraction.text`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "XPW6Isn2RHEo"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Convert text data into TF-IDF weights\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(df['Content'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsSqANr_fuW_",
        "outputId": "77bfc089-b32b-4e53-883a-9244b4ebfe10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1772, 26640)\n"
          ]
        }
      ],
      "source": [
        "print(X.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoF2Ta9gUtb_"
      },
      "source": [
        "__Step 2. Construct the Training and Test Sets__\n",
        "\n",
        "The data is then split randomly into training and testing sets using the `train_test_split` function from `sklearn.model_selection`, with 80% of the data used for training and 20% used for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "VYCdr9FnVCeH"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, df['Category'], test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5MHet_Pf38O",
        "outputId": "f7875d80-d79d-49f9-8eb3-b6378b712655"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1417, 26640) (355, 26640)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape, X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6q5gA0ueXuF7"
      },
      "source": [
        "__Step 3. Model Training__\n",
        "\n",
        "A Bernoulli Naive Bayes model is trained using the `BernoulliNB` class from `sklearn.naive_bayes`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "G6jbNgHLMdiL",
        "outputId": "c6ce23fc-082b-49c7-e7a0-11e37f1da6c0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BernoulliNB()"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.naive_bayes import BernoulliNB\n",
        "\n",
        "# Train the Bernoulli Naive Bayes model\n",
        "bnb = BernoulliNB()\n",
        "bnb.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twAGTz-hMfRm",
        "outputId": "dd904ca6-529b-4be6-a18c-5ba898e147ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BernoulliNB()\n"
          ]
        }
      ],
      "source": [
        "print(bnb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCjhaQYCXrOR"
      },
      "source": [
        "__Step 4. Model Evaluation__\n",
        "\n",
        "The trained model is then used to predict the categories of the test set. The model's performance is evaluated using accuracy and the classification report, which includes precision, recall and F1. These metrics are calculated using the `accuracy_score` and `classification_report` functions from `sklearn.metrics`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQCoe_NRMk9e",
        "outputId": "d8eb8a74-dba6-4018-927b-d787628ea452"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9154929577464789\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.99      0.89       116\n",
            "           1       0.99      0.93      0.96       122\n",
            "           2       0.99      0.82      0.90       117\n",
            "\n",
            "    accuracy                           0.92       355\n",
            "   macro avg       0.93      0.92      0.92       355\n",
            "weighted avg       0.93      0.92      0.92       355\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Predict the categories of the test set\n",
        "y_pred = bnb.predict(X_test)\n",
        "\n",
        "# Print accuracy and classification report\n",
        "print(accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VRZPzGvrPKQ"
      },
      "source": [
        "__Step 5. Interpretation of Results__\n",
        "\n",
        "To interpret the results, it is often useful to compare performance on the training and test sets, and some information is easier to understand in graph form. One Python library useful for generating plots is `matplotlib`.\n",
        "\n",
        "The following is one example usage. \n",
        "* `import numpy as np`: This imports the NumPy library, which is used for numerical operations in Python, here for handling arrays and performing array operations.\n",
        "* `import matplotlib.pyplot as plt`: This imports the plotting library that we use to create the bar plot.\n",
        "* `np.unique(y_test, return_counts=True)`: Here we use NumPy's unique function to find the unique elements of y_test and y_pred (the true labels and the predicted labels), and count their occurrences.\n",
        "* `plt.figure(figsize=(12,6))`: This function creates a new figure for the plot with the specified figure size to make sure all elements fit nicely and are clearly visible.\n",
        "* `plt.bar`: This function is used to create bar plots for both the true and the predicted label distributions. We create two sets of bars, one for the true labels and one for the predicted labels, with different colours for each.\n",
        "* `plt.xticks`: This function is used to set the labels for the x-axis using the category names, and we rotate the labels by 45 degrees to avoid overlapping.\n",
        "* `plt.legend()`: This adds a legend to the plot to differentiate between true and predicted bars.\n",
        "* `plt.tight_layout()`: This function ensures that all elements of the plot are displayed properly without overlapping.\n",
        "* `plt.savefig('topic_distribution.png')`: This function saves the plot as a PNG file.\n",
        "* `plt.show()`: Finally, this function displays the plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "L_iLNMl3qtUY",
        "outputId": "74595062-589d-429e-b454-bab587785e29"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 576x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the distribution of the actual topics in the test set\n",
        "unique_true, counts_true = np.unique(y_test, return_counts=True)\n",
        "\n",
        "# Plot the distribution of the predicted topics\n",
        "unique_pred, counts_pred = np.unique(y_pred, return_counts=True)\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "\n",
        "# Create bar width\n",
        "barWidth = 0.2\n",
        "\n",
        "# Set position of bar on X axis\n",
        "r1 = np.arange(len(counts_true))\n",
        "r2 = [x + barWidth for x in r1]\n",
        "\n",
        "# Create subplot for 'true' distribution\n",
        "plt.bar(r1, counts_true, color='b', width=barWidth, edgecolor='white', label='True')\n",
        "\n",
        "# Create subplot for 'predicted' distribution\n",
        "plt.bar(r2, counts_pred, color='c', width=barWidth, edgecolor='white', label='Predicted')\n",
        "\n",
        "plt.xlabel('Topic', fontweight='bold')\n",
        "plt.ylabel('Frequency', fontweight='bold')\n",
        "plt.title('Frequency Distribution of True and Predicted Topics', fontweight='bold')\n",
        "\n",
        "plt.xticks([r + barWidth/2 for r in range(len(counts_true))], categories, rotation=45)\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('topic_distribution.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feoiaVj1eE_t"
      },
      "source": [
        "## Discussion Questions\n",
        "\n",
        "Experiment with the above code before class and think about the answers to the following questions. It does not matter if you do not do all the questions; the main thing is to do some of them and come to class ready for the discussion.\n",
        "\n",
        "1. **Data Preprocessing**\n",
        "    - Is the order of the steps in data preprocessing important? What happens if you change the order?\n",
        "    - Compare the model's performance on all metrics with and without stopword removal. Are the results much different?\n",
        "    - See if you can find the stopword list used by `nltk`. What do you think of it?\n",
        "    - Find out how to use the English stopword list with `sklearn` and see if the results are better or worse.\n",
        "    - Discuss the advantages and disadvantages of stemming. Would lemmatization be a better option for this problem?\n",
        "2. **Feature Extraction**\n",
        "    - What is the definition of *word* used by `TfidfVectorizer` in `sklearn`? Is there a better definition you can think of? Define the regex.\n",
        "    - Look carefully at the parameters of `TfidfVectorizer`. How do they affect the performance of the models?\n",
        "    - Try replacing `TfidfVectorizer` by `CountVectorizer`, which simply records word counts (term frequencies). Are the results better or worse?\n",
        "3. **Model Training**\n",
        "    - Discuss the parameters of `BernoulliNB`. How do they affect the model's performance?\n",
        "    - Here we chose the Bernoulli Naive Bayes classifier for this problem. Does Multinomial Naive Bayes work better? Which is supposed to work better?\n",
        "    - Are the classes balanced or imbalanced? Is this realistic? Plot the distribution of class labels in the full dataset to show the distribution.\n",
        "    - How well do the Bernoulli Naive Bayes and Multinomial Naive Bayes classifiers work when the classes are imbalanced? Try some experiments.\n",
        "    - This seems to be a particularly \"easy\" classification task. Try the model with more of the 20 categories? Can the good results be replicated?\n",
        "    - In this dataset, each document is given exactly one \"ground truth\" label. How could we handle the situation where some documents have no label?\n",
        "4. **Model Evaluation**\n",
        "    - Discuss the performance of the Bernoulli Naive Bayes model. Is the model performing well? What can be done to improve its performance?\n",
        "    - Here we used only one training-test split. Should we have used cross-validation? Check the `sklearn` documentation to find out how to do this.\n",
        "    - Discuss the evaluation metrics chosen. Are there other metrics that would provide more insights into the performance of the models?\n",
        "    - What do these results tell us about how to build a recommender system based on this model?\n",
        "5. **General Questions**\n",
        "    - Why do we need to classify into topics and subtopics? How well would the recommender system work if we just compared news articles directly?\n",
        "    - What sort of subtopics would be useful with the current classification scheme?\n",
        "    - How might the recommendations differ if we used a collaborative filtering approach instead of a content-based approach? What are the pros and cons?\n",
        "    - Can you think of any other potential applications of text classification and document similarity in the context of news articles?\n",
        "\n",
        "## Advanced\n",
        "\n",
        "1. Research one model not covered in class, such as Logistic Regression, Random Forests, SVM, etc. Which of these methods would be suitable for this topic classification task? Explain why.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
